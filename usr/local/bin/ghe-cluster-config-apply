#!/bin/bash
#/ Usage: ghe-cluster-config-apply [-h]
#/
#/ Validate your /data/user/common/cluster.conf configuration file, copy it to
#/ each node in the cluster and configure each node based on the modified
#/ /data/user/common/cluster.conf file.
#/
#/ OPTIONS:
#/   -h | --help      Show this message.
#/
source /usr/local/share/enterprise/lib/remove-node/nomad-gateway
set -e

usage () {
  grep '^#/' <"$0" | cut -c 4-
}

declare -a passthrough

for arg in "$@"; do
  case "$arg" in
    -h|--help)
      usage
      exit 2
      ;;
    *)
      passthrough+=("$arg")
      ;;
  esac
done

if ghe-config --true config-apply.lightweight.enabled; then
  export GHE_CONFIG_APPLY_FORCE=${GHE_CONFIG_APPLY_FORCE:-""}
else
  export GHE_CONFIG_APPLY_FORCE=${GHE_CONFIG_APPLY_FORCE:-"1"}
fi

export PATH="$PATH:/usr/local/share/enterprise"
export GHE_CONFIG_APPLY_RUN_ID=${GHE_CONFIG_APPLY_RUN_ID:-$(echo $RANDOM | md5sum | head -c 16)}
export GHE_CMD_RUN_ID=${GHE_CMD_RUN_ID:-"$(basename "$0")-$(echo $RANDOM | md5sum | head -c 16)"}

TS_FORMAT='%Y-%m-%dT%H:%M:%S%z'

if [ -z "$HA_CLUSTER" ] && [ -f /etc/github/cluster ] && [ "$(ghe-config "cluster.ha")" = "true" ]; then
  export HA_CLUSTER=1
fi

ghe-license check

if [ ! -f "/etc/github/cluster" ] || [ -z "$(cat /etc/github/cluster)" ]; then
  echo "Clustering is not configured on this host." >&2
  exit 1
fi

# if we are not on a primary node we are on a replica
hostname=$(cat /etc/github/cluster)
if [ "$(ghe-config "cluster.ha")" = true ] && { [ "$(ghe-config cluster."$hostname".replica)" == "enabled" ] || [ "$(ghe-config cluster."$hostname".replica)" == "disabled" ]; }; then
  # if on a replica make sure replication is running before copying config from an un-configured node
  if [ ! -f /etc/github/repl-running ]; then
    echo "WARN: Replication is not running, config apply aborted from a replica appliance which is not ready yet. Exiting without action."
    exit 1
  fi
fi

# Run the NES cluster health check, will not run if NES is disabled
/usr/local/bin/ghe-nes-cluster-health-check || true

ssh_node() {
  local ip=$1
  shift
  ssh -p 122 -oConnectTimeout=2 -oUserKnownHostsFile=/dev/null \
             -oStrictHostKeyChecking=no -oLogLevel=quiet "admin@$ip" "$@"
}

get_node_ip() {
  local node_name="$1"
  if [ -n "$(ghe-config cluster.$node_name.ipv6)" ]; then
    ip=$(ghe-config "cluster.$node_name.ipv6")
  elif [ -n "$(ghe-config cluster.$node_name.ipv4)" ]; then
    ip=$(ghe-config "cluster.$node_name.ipv4")
  fi
  echo $ip
}

# Check connectivity of hosts contained within cluster.conf
if [ "$(ghe-config "cluster.ha")" != "true" ]; then
  hosts=$(ghe-cluster-nodes)
  local_host=$(cat /etc/github/cluster)
  declare -A unreachable_hosts=()
  for hostname in $hosts; do
    ip=$(get_node_ip $hostname)
    if [ -z "$ip" ]; then
      echo "No IP found for cluster.conf entry with hostname \"$hostname\"" >&2
      exit 1
    fi
    node_offline="$(ghe-config "cluster.$hostname.offline" || true)"
    if [ "$hostname" != "$local_host" ] && [ "$node_offline" != "true" ]; then
      set +e
      $(ssh_node "$ip" "echo 0" >/dev/null 2>&1)
      ssh_exit_status=$?
      set -e
      if [ "$ssh_exit_status" != "0" ]; then
        unreachable_hosts["$hostname"]="$ip"
      fi
    fi
  done
  if [ "${#unreachable_hosts[@]}" -gt "0" ]; then
    echo "The following cluster nodes cannot be reached over SSH and are not marked offline:" >&2
    for hostname in ${!unreachable_hosts[@]}; do
      echo "${hostname} (${unreachable_hosts[${hostname}]})"
    done
    echo  "If you've added new nodes, please run ghe-cluster-config-init to initialize them." >&2
    exit 2
  fi
fi

# Check for nodes that were marked offline/deleted, but are still present in nomad
if ghe-config --true app.drain-when-offline.enabled; then
  # Validate the config is correct before trying to calculate diffs
  ghe-cluster-config-check /data/user/common/cluster.conf || {
    echo "Configuration errors must be fixed before running ghe-cluster-config-apply." >&2
    ghe-cluster-config-check /data/user/common/cluster.conf >&2
    exit 2
  }
  # Calculate diffs of removed/offline'd nodes, and update their statuses in nomad
  cluster_diffs=$(ghe-cluster-file-change)
  if [ -n "$cluster_diffs" ]; then
    echo "The following node(s) have been deleted from cluster.conf or marked offline and will be removed from the cluster: $cluster_diffs"
    for node in $cluster_diffs; do
      mark_nomad_node_ineligible "$node"
    done

    for node in $cluster_diffs; do
      drain_nomad_node "$node" --force
    done
  fi
fi

# Env var can be referenced by ghe-single-config-apply to know it's
# running on the local host, for single-host actions
export GHE_CLUSTER_LOCAL_HOST=true
ghe-cluster-each -o -- sudo /usr/local/share/enterprise/ghe-docker-load-images
# if there's a problem setting this, we'll get an error from the ghe-config-apply-script written to console
ghe-cluster-each -o -- /usr/local/share/enterprise/ghe-config-apply-status --reset 2>/dev/null

# Place the files onto the other servers
sudo env HA_CLUSTER=$HA_CLUSTER ghe-cluster-config-update

LOGFILE=/data/user/common/ghe-config.log

# Attempt to upload cluster.conf to Consul via GHES-Manage API if the toggle is enabled
# If we enforce cluster config validation, we should fail if this fails
if [ "$(ghe-config "core.toggle-gcm-cluster-config-validation")" == "true" ]; then
  if [ "$(ghe-config "core.enforce-gcm-cluster-config-validation")" != "true" ]; then
    /usr/local/share/enterprise/ghe-gcm-upload-config 2> >(ts "$TS_FORMAT" | tee -a $LOGFILE >&2) 1> >(ts "$TS_FORMAT" >> $LOGFILE) || true
  else
    /usr/local/share/enterprise/ghe-gcm-upload-config 2> >(ts "$TS_FORMAT" | tee -a $LOGFILE >&2) 1> >(ts "$TS_FORMAT" >> $LOGFILE)
  fi
fi

for phase in 1 2 3 4; do
  echo "Configuration Phase $phase"
  if [ $phase = 3 ]; then
    # Since we may setup replication during config apply, we have to run phase 3 (mysql replication)
    # on mysql-replica-master node before other nodes in replica datacenter

    ghe-cluster-each -o --cluster-ha-primary -- GHE_CMD_RUN_ID="$GHE_CMD_RUN_ID" GHE_CONFIG_APPLY_RUN_ID="$GHE_CONFIG_APPLY_RUN_ID" GHE_CONFIG_APPLY_FORCE="$GHE_CONFIG_APPLY_FORCE" ghe-single-config-apply --phase-$phase "${passthrough[@]}"
    ghe-cluster-each -o --cluster-ha-replica -- GHE_CMD_RUN_ID="$GHE_CMD_RUN_ID" GHE_CONFIG_APPLY_RUN_ID="$GHE_CONFIG_APPLY_RUN_ID" GHE_CONFIG_APPLY_FORCE="$GHE_CONFIG_APPLY_FORCE" ghe-single-config-apply --phase-$phase "${passthrough[@]}"

    # Upload the configuration file to NES (auto-skipped if nes is disabled)
    /usr/local/share/enterprise/ghe-nes-upload-config.rb 2> >(ts "$TS_FORMAT" | tee -a $LOGFILE) 1> >(ts "$TS_FORMAT" >> $LOGFILE) || {
      echo "Error: Failed to upload config to NES.  See $LOGFILE"
    }
  else
    if [ $phase = 2 ]; then
      # Since some additional secrets (e.g. mssql certs) are generated in phase 1, we need to sync these too
      sudo env HA_CLUSTER=$HA_CLUSTER ghe-cluster-config-update -a
    fi

    ghe-cluster-each -o -- GHE_CMD_RUN_ID="$GHE_CMD_RUN_ID" GHE_CONFIG_APPLY_RUN_ID="$GHE_CONFIG_APPLY_RUN_ID" GHE_CONFIG_APPLY_FORCE="$GHE_CONFIG_APPLY_FORCE" ghe-single-config-apply --phase-$phase "${passthrough[@]}"
  fi
done

# Skip if BYODB is enabled because the orchestrator will not be running
if ! ghe-config --true "mysql.external.enabled"; then
  # Re-enable failovers after running configuration.
  if [ -z "$HA_CLUSTER" ] && ghe-config --true "cluster.mysql-auto-failover"; then
    /usr/local/share/enterprise/ghe-orchestrator-client -c submit-masters-to-kv-stores -alias ghe >/dev/null
    /usr/local/share/enterprise/ghe-orchestrator-client -c enable-global-recoveries >/dev/null
  fi
fi

echo "Finished cluster configuration"

# Archive the configuration, if ghe-cluster-config-update hasn't been invoked via ghe-config-apply.
# ghe-cluster-config-apply can be called via ghe-config-apply, which already handles configuration archiving.
PARENT_COMMAND=$(ps $PPID | tail -n 1)
if grep -v "ghe-config-apply" <<< "$PARENT_COMMAND" >/dev/null; then
  # stderr goes to console and appended to LOGFILE, stdout just goes to the LOGFILE
  # both get prepended with the timestamp
  sudo ghe-config-archive -s 2> >(ts "$TS_FORMAT" | tee -a $LOGFILE) 1> >(ts "$TS_FORMAT" >> $LOGFILE)
fi

#!/bin/bash
#/ Usage: ghe-cluster-status-nodes [-hejnv]
#/
#/ Check the status of each node in the cluster.
#/
#/ OPTIONS:
#/   -h | --help      Show this message.
#/   -e | --extended  Perform extended status checks.
#/   -j | --json      JSON formatted output.
#/   -n | --nagios    Nagios formatted output and exit codes.
#/   -v | --verbose   Show verbose output.
#/
# shellcheck source=../share/enterprise/ghe-nomad-lib
source /usr/local/share/enterprise/ghe-nomad-lib
set -e

CHECK_PORTS="
22
80
122
443
1336
1433
3033
3306
5115
6379
8000
8001
8149
8443
9000
9092
9102
9105
9200
9300
11211
"

CHECK_SERVICES="
alambic
elasticsearch
github-ernicorn
grafana
graphite-web
memcached
mssql
mysql
nginx
redis
samplicator
"

# Show usage.
if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
  grep '^#/' < "$0" | cut -c 4-
  exit 2
fi

# shellcheck disable=1091
source /usr/local/share/enterprise/ghe-cluster-status-common

start_output "nodes"

# Usage: require_service <service> (started|stopped)
#   Validate if a specified service matches an expected state ("started" or "stopped") on the currently
#   active host (determined by the value of $hostname).
require_service() {
  local service=$1
  local expected_state=$2
  local status=1
  local node_service_status
  case "$expected_state" in
    started)
      echo "$active_service_list" | grep -q "$service" && status=$?

      # If the service was not found in the active service list, fallback to checking systemd directly.
      if [ "$status" -ne 0 ]; then
        node_service_status=$(ssh_node "$hostname" "sudo systemctl show --property=ActiveState $service" | sed 's,ActiveState=,,')
        echo "$node_service_status" | grep -q "^active" && status=$?
      fi
      ;;
    stopped)
      node_service_status=$(ssh_node "$hostname" "sudo systemctl show --property=ActiveState $service" | sed 's,ActiveState=,,')
      echo "$node_service_status" | grep -Eq '^(inactive|failed|activating)' && status=$?
      ;;
    *)
      check_result "$hostname" "service-$expected_state-$service" "warn" "Unknown service state: '$expected_state'"
      return
  esac

  if [ "$status" -eq 0 ]; then
    check_result "$hostname" "service-$expected_state-$service" "ok"
  else
    check_result "$hostname" "service-$expected_state-$service" "error" "$(ssh_node "$hostname" sudo systemctl show --property=ActiveState "$service" | sed 's,ActiveState=,,')"
  fi
}

post_check_cleanup() {
  if [ "$output" = "json" ]; then
    echo "$JSON" > "$WORKDIR/$BASHPID.json"
  fi

  # Add the counts to a file in the event there is an error or warning.
  # We need this to transfer the error/warning counter between subshells.
  if [ "$error_count" -gt 0 ]; then
    if [ -f "$WORKDIR/error.count" ]; then
      echo $((error_count + $(cat "$WORKDIR/error.count"))) > "$WORKDIR/error.count"
    else
      echo "$error_count" > "$WORKDIR/error.count"
    fi
  fi
  if [ "$warn_count" -gt 0 ]; then
    if [ -f "$WORKDIR/warn.count" ]; then
      echo $((warn_count + $(cat "$WORKDIR/warn.count"))) > "$WORKDIR/warn.count"
    else
      echo "$warn_count" > "$WORKDIR/warn.count"
    fi
  fi
}

check_nomad_jobs() {
  local ok_jobs unhealthy_jobs
  ok_jobs=$(fetch_healthy_jobs)
  for job in $ok_jobs; do
    check_result "nomad" "nomad-job-$job" "ok"
  done

  unhealthy_jobs=$(fetch_unhealthy_jobs)
  for job in $unhealthy_jobs; do
    tasks=$(fetch_unhealthy_tasks "$job")
    for task in $tasks; do
      check_result "nomad" "nomad-job-$job" "error" "$task is not running"
    done
  done

  post_check_cleanup
}

check_ports_services() {
  local hostname=$1
  local ip=

  if ssh_node "$hostname" "echo" >/dev/null; then
    check_result "$hostname" "connect-ssh" "ok"

    # Only perform extensive checks if explicitly requested.
    if [ -n "$extended" ]; then
      if [ -n "$(ghe-config "cluster.$hostname.ipv6")" ]; then
        ip=$(ghe-config "cluster.$hostname.ipv6")
      elif [ -n "$(ghe-config "cluster.$hostname.ipv4")" ]; then
        ip=$(ghe-config "cluster.$hostname.ipv4")
      fi

      for port in $CHECK_PORTS; do
        if nc_out=$(is-port-up "$ip" "$port" 2>&1); then
          check_result "$hostname" "connect-port-$port" "ok"
        else
          check_result "$hostname" "connect-port-$port" "error" "${nc_out//nc.openbsd: /}"
        fi
      done

      node_version=$(ssh_node "$hostname" "bash -c 'source /etc/github/enterprise-release; echo \$RELEASE_VERSION'")
      if [ "$node_version" = "$local_version" ]; then
        check_result "$hostname" "enterprise-version" "ok"
      else
        check_result "$hostname" "enterprise-version" "error" "$local_version expected, found $node_version"
      fi

      # Only capture the active services by setting awk's record separator (RS) to the null string, which is a magic value in awk
      # to represent blank lines.
      active_service_list=$(ssh_node "$hostname" "ghe-service-list" | awk -v RS='' '/^active/' || true)
      require_service haproxy started
      require_service nomad started
      require_service snmpd "$(if ghe-config --true 'snmp.enabled'; then echo started; else echo stopped; fi)"
      require_service syslog-ng started

      for service in $CHECK_SERVICES; do
        if /usr/local/share/enterprise/ghe-service-enabled "$service" "$hostname"; then
          state=started
        else
          state=stopped
        fi
        require_service "$service" "$state"
      done
    fi
  else
    check_result "$hostname" "connect-ssh" "error"
    check_result "$hostname" "enterprise-version" "error" "version check failed"
  fi

  post_check_cleanup
}


local_version=$(bash -c 'source /etc/github/enterprise-release; echo $RELEASE_VERSION')

# Query each host in parallel
pids=
for hostname in $(ghe-cluster-nodes); do
  ( check_ports_services "$hostname" ) &
  pids="$pids $!"
done

[ -n "$extended" ] && {
  check_nomad_jobs &
  pids="$pids $!"
}

# Wait for all background jobs
wait

# Pull in the temp counts from the parallel port and services checks and cleanup.
if [ -f "$WORKDIR/error.count" ]; then
  error_count=$((error_count + $(cat "$WORKDIR/error.count" && rm -f "$WORKDIR/error.count")))
elif [ -f "$WORKDIR/warn.count" ]; then
  warn_count=$((warn_count + $(cat "$WORKDIR/warn.count" && rm -f "$WORKDIR/warn.count")))
fi

if [ "$output" = "json" ]; then
  # Merge all the JSON files recursively into a single valid JSON object
  # Broken down...
  #
  # `jq -s` reads the entire input stream, the content of all the json files,
  #         into a large array and runs the filter on that array just once
  #
  # `reduce .[] as $x ({}; . * $x )` takes the array and for each item in the array `.[]`,
  #                                  `. * $x` is run to recursively add them to an object,
  #                                  starting with an empty object denoted by `{}`.
  JSON=$(jq -s 'reduce .[] as $x ( {}; . * $x )' "$WORKDIR/"*.json)
  for pid in $pids; do
    rm -f "$WORKDIR/$pid.json"
  done
  # This same directory is used by other scripts when run via `ghe-cluster-status` so we
  # only delete the directory if it is empty, in the event this script is run in isolation.
  rmdir "$WORKDIR" > /dev/null 2>&1 || true
fi

end_output "nodes"
